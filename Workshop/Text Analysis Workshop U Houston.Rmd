---
title: "Intro to Text Analysis (Part 1)"
author: "Sebastián Vallejo"
date: "12/5/2019"
output: 
  html_document:
    toc: true
    number_sections: true  
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/sebastian/Box Sync/UH/Text Analysis Workshop/Twits')
```

# Outline of the workshop

1. Text as data: Brief overview of the theory behind text analysis
2. The corpus
3. Analyzing our corpora
  + Features
  + Dictionary-based approaches
  + Estimating distance
  + Topic models
  + Natural language processing (NLP)
  + Text networks
  + Neural networks / word embeddings
4. Conclusion and parting remarks

Note: The point of this workshop is not to for you to leave an expert on text analysis, but rather for you to have a taste of what can be achieved (i.e. what substantive questions can be answered) using text analysis techniques. The code provided can help you get started, but you will need to explore each method more in detail if you want to apply it to your research.  

## What will you need?

If you want to follow along in your computer, you should have spaCyR [intalled](https://github.com/vallejo086/Text-Analysis-Workshop-U-Houston/blob/master/Installing%20spaCyR/Installing_spaCyR.md). In addition to spaCyR, you should have the following packages installed:

- quanteda
- [quanteda.dictionaries](https://rdrr.io/github/kbenoit/quanteda.dictionaries/f/README.md)
- stm
- tidyverse 
- ggplot2
- lme4
- lattice
- ggthemr (to prettify)
- dplyr

Text Analysis is a computer-intensive task. spaCyR and quanteda processes can consume a lot of your RAM, so take that into account when running your code. 

## What are my sources?

Much of the material/ideas for the workshop were taken from the following sources:

- [Kenneth Benoit's website](http://kenbenoit.net/quantitative-text-analysis-tcd-2018/)
- The [spaCyR GitHub page](https://github.com/quanteda/spacyr)
- [Chris Bail's Course](https://cbail.github.io/textasdata/Text_as_Data.html)
- [Elliot Ash's Course](https://github.com/ellliottt/text_ml_course_2018/blob/master/slides/Text-class-01-intro.pdf)
- Will Lowe's Text Analysis workshop at IQMR
- [The Internet](https://en.wikipedia.org/wiki/Internet) 

If you are interested in applied and/or theoretical readings on text analysis, here is a short list to get you started:

- [Grimmer and Stewart (2013)](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20) - Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts **in Political Analysis**
- [Lucas et al. (2015)](https://www.cambridge.org/core/journals/political-analysis/article/computerassisted-text-analysis-for-comparative-politics/CC8B2CF63A8CC36FE00A13F9839F92BB) - Computer-Assisted Text Analysis for Comparative Politics **in Political Analysis**
- Blei (2012) - Probabilistic Topic Models **in Communications of the ACM**
- [**Poetics**](http://www.sciencedirect.com/science/journal/0304422X/41/6), Volume 41, Special Issue on "Topic Models and the Cultural Sciences"
- [Slapin and Proksch (2008)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2008.00338.x?casa_token=YA_EuVuWcyoAAAAA:fEdUKJiG91D6jF9iIfDXquhQM-CBeDiUurC4KPrV52m8YV6leKoEWykzdOxh0mD_v94Y-qw4VEHpoWo) - A Scaling Model for Estimating Time-Series Party Positions from Texts **in AJPS**
- [Welbers et al. (2017)](https://www.tandfonline.com/doi/abs/10.1080/19312458.2017.1387238) - Text Analysis in R **in Communication Methods and Measures**

Let's start!

# Text as data: Some theory

## Why text?

1. Politicians *love* to speak. 
2. Bureaucrat *love* to write. 
3. Machirulos *love* to tweet. 

There is text in every aspect of politics: debates on legislation, peace treaties, news reports, political manifestos, campaign speeches, social media, etc. Not only is text ubiquitous, but it is produced at the time (sometimes in real time). 

## Why use the help of a computer?

1. Humans are great at understanding and analyzing the content of texts. Computers are not. 
2. Humans are also great at not being able to read thousands of documents in minutes, organize that text, classify that text, scale that text, and then produce pretty graphics from that text. Computers are not not that. (I stand by the double negative)

We will be learning about the latter.

Note: Text analysis is not a field, it is a tool. Think about it as a regression where the data are words instead of numbers. Thus, the fanciest of text analysis techniques is no good without a well thought out, substantive, theoretically motivated question. 

## Four principles of automated text analysis (from Grimmer and Stewart (2013))

1. All Quantitative Models of Language Are Wrong—But Some Are Useful.
  + Data generation process for text is unknown.
  + Language it too complex for computers to correctly decipher (e.g. "Time flies like an arrow. Fruit flies like a banana."). 
  + Since language is so context-specific, more complex models are rarely more useful for the analysis of texts.
2. Quantitative Methods Augment Humans, Not Replace Them
  + You need to read the text, know the text, be the text. 
  + Computers organize, direct, and suggest. 
  + Humans read and interpret. 
3. There Is No Globally Best Method for Automated Text Analysis
  + Different needs require different methods. 
  + Even when you have the same needs, the same model might not fit the data.
4. Validate, Validate, Validate
  + Outputs can be misleading (or simply wrong). 
  + It is incumbent upon you, the researcher, to validate the use of automated text analysis.

Now we can start with some (sample) text analysis. Note that we will be doing basic analysis (as generalizable as it gets) of text, and then use some models as examples. To see the extent of what can be done with different models, I suggest you read [Grimmer and Stewart (2013)](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20). 

# The corpus (in theory)

To analyze text, we first need a corpus, a large and structured set of texts for analysis. The units (texts) of the "structured set of texts" can be anything you want them to be: a complete speech, each paragraph of the speech, or each sentence within that paragraph. The relevance of the unit will depend on your research question. 

## Text as data: Some specifics

Text data is a sequence of characters called **documents**. The set of documents is the **corpus**. 

- Text data is **unstructured**
  + There is information we want, mixed in with (A LOT) of information we do not. 
  + We need to separate the wheat from the chaff.
  
Note: All text analysis methods will produce some information. The *art* lies on the ability of the researcher to figure out what is valuable and what is not. 

## What counts as a document?

The unit of analysis when using text as data will depend on the question you are asking. For example:

- If you are looking at how politicians react to different types of economic crises, then each document produced after a crisis would be the unit of analysis.
- If you are looking at how politicians differ within a campaign, then you might aggregate all the texts produced by a candidates during a campaign as the unit of analysis.
- If you are looking at how politicians address different topics within a campaign, then your unit of analysis might be a section or paragraph of a manifesto. 

## Where can I get my hands on some juicy corpora?

1. Chris Bail curates a [list](https://docs.google.com/spreadsheets/d/1I7cvuCBQxosQK2evTcdL3qtglaEPc0WFEs6rZMx-xiE/edit#gid=0) of corpora already compiled and ready to use.
2. Governments produce text ALL THE TIME. It is, usually, publicly available and, depending on the country/organization, easily accessible. 
3. Scrape the webs. Websites can make it difficult to scrape data with restrictive terms of use (i.e. bot-blockers or, worse, javascript). There are creative ways to get around these. (If you are interested in web-scrapping, let me know and I can help you get started with some code.)

## Some final words on documents

Original corpora are rarely ready to be analyzed without some previous cleaning. You often want to get rid of hyphenations at line brakes, table of contents, indexes, etc. All of these are corpus-specific and require attention ahead of time. 

- Learn how to use regular expressions (regex). The `stringr` package in R or the Python package *re* are useful tools. 
- While useful, regex is tedious to learn. Check out [Sanchez 2013](https://github.com/vallejo086/Text-Analysis-Workshop-U-Houston/blob/master/Sanchez%20(2013)%20-%20Handling%20and%20Processing%20Strings%20in%20R.pdf) for a good guide. 

Some data are only available as non-searchable PDFs or images. These need to be converted to text before R (or Python) can read them. I use ABBYY FineReader, which is 'expensive' but might be available at your (old/next) university library. Joe Sutherland has an [open-source OCR](https://github.com/jlsutherland/doc2text) (Optical Character Recognition).  

Finally, I advise against using spell checkers. Most corpora use specialized language that would be flagged by standard spell-checkers (and I have not found one that can be automated to check text in Spanish). In most empirical contexts, we can safely assume that spelling errors (especially OCR errors) are uncorrelated with treatment assignment. 

(Ask me about some of the other strengths weaknesses of using data from social media.)

# The corpus (in practice)

We need texts for text analysis. Luckily, we have a [dataset](https://github.com/vallejo086/Text-Analysis-Workshop-U-Houston/blob/master/Capitalism%20Dataset/data_capitalism.Rdata) of 5640 tweets from 3885 unique users containing the word 'capitalism'. The dataset has been [tinkered with and cleaned](https://github.com/vallejo086/Text-Analysis-Workshop-U-Houston/blob/master/Capitalism%20Dataset/Downloading%20Tweets%20and%20Cleaning%20the%20Data) and is ready to be processed. 

Before we start, let's load the required packages. Remember that spaCyR is a Python wrapper that needs to first be initialized. 

```{r results = 'hide'}
rm(list=ls(all=TRUE))
library(tidyverse)
library(spacyr)
library(stm)
library(quanteda)
library(quanteda.dictionaries)
library(dplyr)
library(lattice)
library(ggplot2)
library(ggrepel)
library(ggthemr)
ggthemr("fresh")
spacy_initialize()
```

Let's load our dataset and see how it looks. 

```{r}
load("data_capitalism.Rdata")
data_capitalism <- data_capitalism[!data_capitalism$text_clean=="",]
data_capitalism %>% glimpse()
```

To analyze the text found in the dataset, we need to create a `corpus` object. The `corpus()` function (from the quanteda package) does just this. The main object has to be a character vector. (The `readtext()` function from the readtext package can read text-formatted files). 

```{r}
cap_corp <- corpus(data_capitalism$text_clean, 
                       docvars = data.frame(author = data_capitalism$nameauth,
                                            time = data_capitalism$timeRT,
                                            in_degree = data_capitalism$to_ind,
                                            followers = data_capitalism$followersRT,
                                            RT_count = data_capitalism$retweetRT,
                                            left_right = data_capitalism$mem_name,
                                            left_right_dum = data_capitalism$mem_name_dummy),
                       metacorpus = list(source = "Twitter",
                                         notes = "Scrapped on Nov. 30 - Dec. 3, 2019"))

summary(cap_corp,10) 
```

A corpus object works similar to a normal dataset, in that each document (in our case, each tweet) is an observation that has additional covariates (i.e. *docvars*) that describe it. 

```{r}
cap_corp[c(1:5)]
```

You can explore a corpus as you would explore other lists (with brackets). To get all the texts you can `texts(cap_corp)`, but you don't want to do that. We can subset a corpus. Here are all the tweets from TW authorities with more than 90K followers (and less than 200 words... I know, useless, but you might appreciate the code to do it): 

```{r}
cap_subset <- corpus_subset(cap_corp, 
                                followers > 90000 & ntype(cap_corp) < 200)
summary(cap_subset, 15)
```

We might be interested in sentences rather than complete tweets. 

```{r}
cap_sentences <- corpus_reshape(cap_corp, to = "sentences") # or "paragraphs"
summary(cap_sentences,8)
cap_sentences[1:5]
```

Since these are tweets and people often tweet one sentence at a time, this conversion might be moot for our corpus. But in longer texts narrowing down the unit of analysis can be helpful, especially if we are trying to estimate topic models (more on this later). 

The reshape function can divide texts into "sentences" and "paragraphs". `corpus_reshape()` uses punctuation marks (e.g. "\\\\n", "\\n") to determine cuts. 

# Pre-processing the corpus

As previously mentioned, our corpora have the information we want, and a lot of information we do not. Uninformative data add noise and reduce the precision of resulting estimates (and are computationally costly). We aim to have a "bag-of-words", or to convert a corpus *D* to a matrix *X*. In the "bag-of-words" representation, a row of *X* is just the frequency distribution over words in the document corresponding to that row. 

Before we do that, we will get rid of all the unwanted information. First, we turn all words to lower-case and get rid of all punctuation and numbers. The `tokens()` command will do this and separate all the texts into tokens. 

```{r}
cap_toks <- tokens(cap_corp,
                   remove_numbers = T,
  remove_punct = T,
  remove_separators = TRUE,
  remove_twitter = T,
  remove_hyphens = T,
  remove_url = T)
```

**Tokens** (`ntoken`) is a fancy name for "word". These contain all the information we need to run our models. Yet, there is still a lot of noise. 

```{r}
cap_coll <- textstat_collocations(cap_toks)
head(cap_coll, 20)
```

(Quick detour: Collocations bundle together a set number of words -also known as ngrams- that appear next to each other. The default is 2, but we can set it at any length we want.)

Some words are "useless". Let's get rid of the *stopwords*. (For a take on when stopwords are informative, check [Pennebaker (2011)](http://www.secretlifeofpronouns.com).)

```{r}
cap_toks_stop <- tokens_remove(cap_toks, 
                              stopwords(language = "en"),
                              padding = F)
cap_toks_stop <- tokens_remove(cap_toks_stop, "amp")
cap_coll <- textstat_collocations(cap_toks_stop)
head(cap_coll, 20)
```


<!-- Or maybe we want a three-word collocation -->

<!-- ```{r} -->
<!-- cap_coll3 <- textstat_collocations(cap_toks, size=3) -->
<!-- head(cap_coll3, 20) -->
<!-- ``` -->

<!-- For references, the $\lambda$ score is a measure of the times K specific consecutive tokens happen (in our second example, $K=3$), given all the K consecutive tokens possibilities.  -->

Finally, we might want to stem our tokens. 

![Example of stemming](https://devopedia.org/images/article/218/8583.1569386710.png)

```{r}
cap_toks_stem <- tokens_wordstem(cap_toks_stop)
cap_coll <- textstat_collocations(cap_toks_stem)
head(cap_coll, 20)
```

## Before the "bag-of-words"

We have *just* pre-processed the data, but the number of documents (e.g. tweets) and the (pre-processed) length of these documents already provide an interesting set of variables for analysis. 

For example:
- How do major capitalist events affect the production of tweets from capitalists and anti-capitalists?
- What is the relation between in-degree and effort and quality of tweets? 

```{r}
# How do major capitalist events affect the production of tweets from capitalists and anti-capitalists?

data_capitalism <- data_capitalism %>% 
  group_by(date_created,mem_name_dummy) %>%
   mutate(count_tweets = n())

data_capitalism_res <- data_capitalism[!duplicated(data_capitalism[c(11,13)]),]
data_capitalism_res <- data_capitalism_res[data_capitalism_res$date_created > "2019-11-19",]

ggplot(data_capitalism_res, aes(x=date_created , y = count_tweets , color = mem_name_dummy)) +
         stat_smooth() +
    scale_color_discrete(name = "") +
  labs(x = "Date", y = "Count of Tweets") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position="bottom") +
  geom_vline(xintercept = as.Date("2019-11-29"), color = "black", linetype = "dashed") 
  
# What is the relation between in-degree and effort of tweets? 

data_capitalism$length_tweet <- ntoken(char_tolower(data_capitalism$text_clean),
                                       remove_punct = TRUE)

data_capitalism$length_type <- ntoken(char_tolower(data_capitalism$text_clean),
                                       remove_punct = TRUE) #repeated words are cut..

ggplot(data_capitalism, aes(x=to_ind , y =  length_tweet)) +
         geom_point() +
     geom_smooth(method = "lm", se = T)+
  labs(x = "In-Degree", y = "Length of Tweets")  

ggplot(data_capitalism, aes(x=to_ind , y =  length_type)) +
         geom_point() +
     geom_smooth(method = "lm", se = T)+
  labs(x = "In-Degree", y = "Number of Types per Tweets")  

```

# Analyzing our corpus: Keywords in context and collocations

Finally, we are ready to analyze our corpus. We will start by the basic: keywords in context. 

## Keywords in context 

Say we are interested in the way capitalist and anti-capitalist describe **capitalism**. Let's see how they talk about **capitalism** using the "keyword-in-context" function:

```{r}
cap_talk <- kwic(cap_toks_stop, "capital*", window=10) # 10 words before and after the word in question
head(cap_talk, 20)
```

If we put together all these words in one whole text, we can see how they are related in that context.

```{r}
cap_talk_context <- paste(cap_talk$pre, cap_talk$post, collapse = " ")
cap_talk_coll <- textstat_collocations(cap_talk_context, size = 3)
head(cap_talk_coll, 20)
```

As a reference, the $\lambda$ score is a measure of the times K specific consecutive tokens happen (in our second example, $K=3$), given all the K consecutive tokens possibilities.

# Analyzing a corpus: Features

The features of a corpus can give us clues about the characteristics of our text. At this point, we are already treating our documents as "bags-of-words". Having tokenized our corpus means that we are interested in each word, how often they appear, in conjunction with what other words, etc. We are going to creaate a Document-Feature Matrix (*dfm*) object. I will going into detail of what is a *dfm*. For now, just think of it a dataset where each row if the count of words of each document. 

```{r}
dfm_toks <- dfm(cap_toks_stop)
```

Note that my *dfm* object contains all the document level variables I added to my corpus at the beginning. 

## (10) Most frequently used words

Simply: 

```{r}
cap_freq <- textstat_frequency(dfm_toks, n = 5, groups = "left_right")
head(cap_freq, 15)
```

Or a plot of the most frequently used words by group:

```{r}
dfm_toks %>% 
  textstat_frequency(n = 10,groups = "left_right") %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency, color = group)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency")
```

## Lexical diversity

We might be interested in the breadth and variety of vocabulary used in a document. *Lexical diversity* is widely believed to be an important parameter to rate a document in terms of textual richness and effectiveness. Knowing Marxists, we might expect them to use more complex language than capitalists.

```{r}
dfm_toks_2 <- dfm(cap_toks)

cap_lexdiv <- textstat_lexdiv(dfm_toks_2,measure = "TTR")
tail(cap_lexdiv, 5) # Does not tell us much, but...

to_plot <- cbind.data.frame(data_capitalism, cap_lexdiv)

ggplot(to_plot, aes(x= factor(mem_name), y=TTR)) +
  geom_boxplot() +
    labs(x = "Type")  


```

There are many measures of lexical diversity, each measuring something slightly different. `textstat_lexdiv()` includes many and you can check which adapts best to your needs [here](https://rdrr.io/cran/quanteda/man/textstat_lexdiv.html).

## TF-IDF

Looking at simple frequencies might hide some important document features. As in everything in the social sciences, we can always complicate it a bit more. Enter TF-IDF: "Term-frequency / Inverse-document-frequency". TF-IDF weighting up-weights relatively rare words that do not appear in all documents. Using term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents.  

```{r}
head(dfm_toks[, 5:15])

head(dfm_tfidf(dfm_toks)[, 5:15])
```

If we are building a dictionary, for example, we might want to include words with high TF-IDF values. Another way to think about TF-IDF is in terms of predictive power. Words that are common to all documents do not have any predictive power and receive a TD-IDF value of 0. Words that appear, but only in relatively few document, have greater predictive power and receive a TD-IDF > 0. 

## Wordclouds

Wordclouds are silly, but people seem to **love** them. Begrudgingly, I include the code:

```{r}
textplot_wordcloud(dfm_toks, comparison = F, max_words = 300)

```

# Analyzing a corpus: Dictionary-based approaches

Dictionaries help us connect qualitative (concepts) and quantitative information extracted from text. Constructing a dictionary requires contextual interpretations. The **key** in a dictionary for text analysis (more of a thesaurus) is associated with non-exclusive terms (**values**):

- WC = wc, toilet, restroom, loo, bathroom
- vote = poll, suffrage, franchise, ballot, vote

Formally, there are three major categories:

- Corpus-specific. For example, Pearson and Dancey (2011) use a dictionary category "women,"  which includes mentions of woman, women, woman’s, women’s, girl, girl’s, girls, girls’, female, female’s, females, females’, servicewoman, and servicewomen. 
- General (e.g. LIWC)
- Sentiment Analysis

You can create your dictionary (using the `dictionary()` function), or you can use well-known dictionaries like: General Inquirer (Stone et al. 1996), Regressive Imagery Dictionary (Martindale, 1975, 1990), [Linguistic Inquiry and Word Count](http://liwc.wpengine.com), [Laver and Garry (2000)](https://www.jstor.org/stable/2669268?casa_token=2nTHg75lKHgAAAAA:E5sumoKaeMUgm79-RYpKAF8GIop6bl8bDLSTAwiwBNqKjYRaetPF9_yLQ7SPeE8PlTEfFE5lN2kNaoJESIs9d3aGWjnaEDuADumxYkA3kjeGcDawtw16&seq=1#metadata_info_tab_contents) to distinguish policy domains, Lexicoder Sentiment Dictionary (Young and Soroka, 2012). All dictionaries have drawbacks and may or may not adequately capture what you want them to capture. Remember: validate, validate, validate. 

## Corpus-specific dictionary

Let's apply the dictionary used by Pearson and Dancey (2011) to our corpus and see if there is any gender element to the way each side addresses capitalism. 

```{r}

dict_women <- dictionary(list(mujeres = c("woman", "women", "girl*", "female*")))

cap_women <- liwcalike(data_capitalism$text_clean, 
                               dictionary = dict_women)

to_plot <-  cbind.data.frame(cap_women,data_capitalism)
to_plot$women_dum <- 0 
to_plot$women_dum[to_plot$mujeres>0] <- 1

ggplot(to_plot, aes(x=mem_name, y = mujeres)) +
    geom_boxplot() +
    labs(x = "Type") 

ggplot(to_plot, aes(x=mem_name, y = women_dum)) +
    geom_bar(stat = "identity") +
    labs(x = "Type", y = "Times Women are Mentioned" ) 
```

"The patriarchy is effing vast..."

## LIWC and Sentiment Analysis 

Let's do some simple "sentiment analysis" using two dictionaries: GI and NRC. 

```{r}
cap_sentimentNRC <- liwcalike(cap_corp, 
                               dictionary = data_dictionary_NRC)

cap_sentimentGI <- liwcalike(cap_corp, 
                               dictionary = data_dictionary_geninqposneg)

head(cap_sentimentNRC, 15)
head(cap_sentimentGI, 15)
```

Both produce dataframe objects that can later be manipulated to conduct more in-depth analysis. Let's see how "anger" and "joy" language varies across groups and time (XXX maybe something else) using the NRC dictionary.

```{r}

cap_sentiment_df <- cbind.data.frame(cap_sentimentNRC,data_capitalism)
cap_sentiment_df <- cap_sentiment_df[cap_sentiment_df$date_created > "2019-11-19",]

ggplot(cap_sentiment_df, aes(x=date_created, y=anger, color = mem_name))+
  stat_smooth() +
  labs(title="Anger in Capitalism", x="Date", y = "Anger", las=2)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(cap_sentiment_df, aes(x=date_created, y=joy, color = mem_name))+
  stat_smooth() +
  labs(title="Joy in Capitalism", x="Date", y = "Joy", las=2)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

One of the limitations of sentiment analysis is the variation we get on similar sentiments when using different dictionaries. For example, analyzing *polarity* in our data (positive language minus negative language), we obtain different results from GI and NRC. 

```{r}
polarity_NRC <- cap_sentimentNRC$positive - 
cap_sentimentNRC$negative

polarity_GI <- cap_sentimentGI$positive - 
cap_sentimentGI$negative

cor(polarity_NRC,polarity_GI)

```

As a general recommendation, it is always good practice to explore the dictionary we are going to use before actually using it. This is particularly important when working with text in another language. Most dictionaries (commercial or otherwise) that have been evaluated and validated, have been tested and evaluated in English. If using another language, it might be a good idea to check how words are classified. Another common practice is to use Google Translate API or Microsoft Translator API on the original corpus. This can be costly depending on the size of the corpus and the results, like with any other text-analysis tool, need to be constantly validated (Lucas et al. 2015).   

# Analyzing a corpus: Topic models and document distance

Topics models were primarily developed to summarize unstructured text, use words within documents to infer their topic, and as a form of dimension reduction. It allows social scientists to use topics as a form of measurement, as were are often interested in how observed covariates drive trends in language. For example, 

- Political attention patterns in Senate floor speeches [(Quinn et al. 2010)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2009.00427.x.)
- Attention senators allocate to press releases [(Grimmer 2010)](https://www.cambridge.org/core/journals/political-analysis/article/bayesian-hierarchical-topic-model-for-political-texts-measuring-expressed-agendas-in-senate-press-releases/74F30D05C220DB198F21FF5127EB7205). 
- Climate change "skepticism" in reports and communications by think tanks and interest groups [(Bousaills and Coan 2016)](https://www.sciencedirect.com/science/article/abs/pii/S0959378015300728).

Topic models are "unsupervised" methods. As such, they require a great deal of human supervision, especially when it comes to validating the results. 

## Document-term Matrix

Topic models are a broad class of Bayesian generative models that encode problem-specific structure into an estimation of categories [(Grimmer and Stewart 2013](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20); [Blei et al. 2010)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122269/). Statistically, a topic is a probability mass function over words. The idea of topic models is that each document exhibits a topic in some proportion: each document is a distribution over topics, and each topic is a distribution over words. 

We can take our corpus and turn it into a matrix that reflects this concept. We call it a document-term matrix (*dtm*). 

\begin{table}[]
\centering
\caption{Document-term Matrix X}
\begin{tabular}{c|c|c|c|c}
\hline
  & $W_1$ & $W_2$ & $W_3$ & $W_m$ \\ \hline
$D_1$ & 1 & 0 & 4 & 5 \\ \hline
$D_2$ & 2 & 1 & 0 & 0 \\ \hline
$D_3$ & 0 & 4 & 1 & 2 \\ \hline
$D_n$ & 1 & 1 & 3 & 0 \\ \hline
\end{tabular}
\end{table}

- A corpus of $n$ documents $D_1$, $D_2$, $D_3$ ... $D_n$
- Vocabulary of $m$ words $W_1$, $W_2$, $W_3$ ... $W_m$
- The value of $i,j$ cell gives the frequency count of word $W_j$ in document $D_i$ 

Latent Dirichlet Allocation (LDA) converts the *dtm* into two lower-dimension maatrices, $M_1$ and $M_2$:

\begin{table}[]
\centering
\caption{Matrix $M_1$}
\begin{tabular}{c|c|c|c|c}
\hline
  & $K_1$ & $K_2$ & $K_3$ & $K_p$ \\ \hline
$D_1$ & 1 & 0 & 0 & 1 \\ \hline
$D_2$ & 1 & 1 & 0 & 0 \\ \hline
$D_3$ & 1 & 0 & 1 & 1 \\ \hline
$D_m$ & 1 & 0 & 0 & 0 \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Matrix $M_2$}
\begin{tabular}{c|c|c|c|c}
\hline
  & $W_1$ & $W_2$ & $W_3$ & $W_m$ \\ \hline
$K_1$ & 1 & 0 & 1 & 1 \\ \hline
$K_2$ & 0 & 1 & 0 & 0 \\ \hline
$K_3$ & 1 & 1 & 1 & 0 \\ \hline
$K_p$ & 1 & 0 & 0 & 0 \\ \hline
\end{tabular}
\end{table}

- $M_1$ is a $N x K$ document-topic matrix
- $M_2$ is a $K x M$ topic-term matrix

LDA estimates the distribution over words for each topic, and the proportion of a topic in each document. You can read about the math behind the two-step process in [Grimmer and Stewart (2013)](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20). 

![Plate notation of Latent Dirichlet Allocation](https://wiki.ubc.ca/images/e/ee/LDA_Graphical_Model.jpg)

- $\alpha$ --> document-topic density: higher $\alpha$ means documents contain more topics, lower $\alpha$ means a document contains fewer topics. 
- $\beta$ --> topic-word density. higher $\beta$ means topics have more words, lower $\beta$ means topics have fewer words.
- Number of topics --> this is specified in advance, or can be chosen to optimize model fit (we will get back to this point). The "statistically optimal" topic count is usually too high for the topics to be interpretable or useful. 

 